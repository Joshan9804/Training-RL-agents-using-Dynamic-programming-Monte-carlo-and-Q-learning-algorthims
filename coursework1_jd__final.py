# -*- coding: utf-8 -*-
"""CourseWork1_JD _Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K5MpWxDsYp5bT27SsMhfIjqI2eVJ_18e
"""

import numpy as np 
import random
import matplotlib.pyplot as plt # Graphical library
#from sklearn.metrics import mean_squared_error # Mean-squared error function

"""# Task 1 :
See pdf for instructions. 
"""

def get_CID():
  return "02182106" # Return your CID (add 0 at the beginning to ensure it is 8 digits long)

def get_login():
  return "jd22"

"""## Helper class"""

# This class is used ONLY for graphics
# YOU DO NOT NEED to understand it to work on this coursework

class GraphicsMaze(object):

  def __init__(self, shape, locations, default_reward, obstacle_locs, absorbing_locs, absorbing_rewards, absorbing):

    self.shape = shape
    self.locations = locations
    self.absorbing = absorbing

    # Walls
    self.walls = np.zeros(self.shape)
    for ob in obstacle_locs:
      self.walls[ob] = 20

    # Rewards
    self.rewarders = np.ones(self.shape) * default_reward
    for i, rew in enumerate(absorbing_locs):
      self.rewarders[rew] = 10 if absorbing_rewards[i] > 0 else -10

    # Print the map to show it
    self.paint_maps()

  def paint_maps(self):
    """
    Print the Maze topology (obstacles, absorbing states and rewards)
    input: /
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders)
    plt.show()

  def paint_state(self, state):
    """
    Print one state on the Maze topology (obstacles, absorbing states and rewards)
    input: /
    output: /
    """
    states = np.zeros(self.shape)
    states[state] = 30
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders + states)
    plt.show()

  def draw_deterministic_policy(self, Policy):
    """
    Draw a deterministic policy
    input: Policy {np.array} -- policy to draw (should be an array of values between 0 and 3 (actions))
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze
    for state, action in enumerate(Policy):
      if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action
        continue
      arrows = [r"$\uparrow$",r"$\rightarrow$", r"$\downarrow$", r"$\leftarrow$"] # List of arrows corresponding to each possible action
      action_arrow = arrows[action] # Take the corresponding action
      location = self.locations[state] # Compute its location on graph
      plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph
    plt.show()

  def draw_policy(self, Policy):
    """
    Draw a policy (draw an arrow in the most probable direction)
    input: Policy {np.array} -- policy to draw as probability
    output: /
    """
    deterministic_policy = np.array([np.argmax(Policy[row,:]) for row in range(Policy.shape[0])])
    self.draw_deterministic_policy(deterministic_policy)

  def draw_value(self, Value):
    """
    Draw a policy value
    input: Value {np.array} -- policy values to draw
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze
    for state, value in enumerate(Value):
      if(self.absorbing[0, state]): # If it is an absorbing state, don't plot any value
        continue
      location = self.locations[state] # Compute the value location on graph
      plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph
    plt.show()

  def draw_deterministic_policy_grid(self, Policies, title, n_columns, n_lines):
    """
    Draw a grid representing multiple deterministic policies
    input: Policies {np.array of np.array} -- array of policies to draw (each should be an array of values between 0 and 3 (actions))
    output: /
    """
    plt.figure(figsize=(35,20))
    for subplot in range (len(Policies)): # Go through all policies
      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy
      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze
      for state, action in enumerate(Policies[subplot]):
        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action
          continue
        arrows = [r"$\uparrow$",r"$\rightarrow$", r"$\downarrow$", r"$\leftarrow$"] # List of arrows corresponding to each possible action
        action_arrow = arrows[action] # Take the corresponding action
        location = self.locations[state] # Compute its location on graph
        plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph
      ax.title.set_text(title[subplot]) # Set the title for the graph given as argument
    plt.show()

  def draw_policy_grid(self, Policies, title, n_columns, n_lines):
    """
    Draw a grid representing multiple policies (draw an arrow in the most probable direction)
    input: Policy {np.array} -- array of policies to draw as probability
    output: /
    """
    deterministic_policies = np.array([[np.argmax(Policy[row,:]) for row in range(Policy.shape[0])] for Policy in Policies])
    self.draw_deterministic_policy_grid(deterministic_policies, title, n_columns, n_lines)

  def draw_value_grid(self, Values, title, n_columns, n_lines):
    """
    Draw a grid representing multiple policy values
    input: Values {np.array of np.array} -- array of policy values to draw
    output: /
    """
    plt.figure(figsize=(35,20))
    for subplot in range (len(Values)): # Go through all values
      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value
      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze
      for state, value in enumerate(Values[subplot]):
        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value
          continue
        location = self.locations[state] # Compute the value location on graph
        plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph
      ax.title.set_text(title[subplot]) # Set the title for the graoh given as argument
    plt.show()

"""## Maze class"""

# This class define the Maze environment

class Maze(object):

  # [Action required]

  def get_CID():
    return "02182106" # Return your CID (add 0 at the beginning to ensure it is 8 digits long)

  def get_login():
    return "jd22"
  def __init__(self):
    """
    Maze initialisation.
    input: /
    output: /
    """
    
    # [Action required]
    # Properties set from the CID
    
    self._prob_success =0.8 + 0.02 *(9-float(get_CID()[-2])) #0.8 + 0.02 × (9 − y). # float = 0.98

    self._gamma = 0.8 + 0.02*float(get_CID()[-2])  # The discount factor γ is set as γ = 0.8 + 0.02 × y. 0.8
    self._goal = int(get_CID()[-1]) % 4 # integer (0 for R0, 1 for R1, 2 for R2, 3 for R3)

    # Build the maze
    self._build_maze()
                              

  # Functions used to build the Maze environment 
  # You DO NOT NEED to modify them
  def _build_maze(self):
    """
    Maze initialisation.
    input: /
    output: /
    """

    # Properties of the maze
    self._shape = (13, 10)
    self._obstacle_locs = [
                          (1,0), (1,1), (1,2), (1,3), (1,4), (1,7), (1,8), (1,9), \
                          (2,1), (2,2), (2,3), (2,7), \
                          (3,1), (3,2), (3,3), (3,7), \
                          (4,1), (4,7), \
                          (5,1), (5,7), \
                          (6,5), (6,6), (6,7), \
                          (8,0), \
                          (9,0), (9,1), (9,2), (9,6), (9,7), (9,8), (9,9), \
                          (10,0)
                         ] # Location of obstacles
    self._absorbing_locs = [(2,0), (2,9), (10,1), (12,9)] # Location of absorbing states
    self._absorbing_rewards = [ (500 if (i == self._goal) else -50) for i in range (4) ]

    self._starting_locs = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9)] #Reward of absorbing states
    self._default_reward = -1 # Reward for each action performs in the environment
    self._max_t = 500 # Max number of steps in the environment

    # Actions
    self._action_size = 4
    self._direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on
        
    # States
    self._locations = []
    for i in range (self._shape[0]):
      for j in range (self._shape[1]):
        loc = (i,j) 
        # Adding the state to locations if it is no obstacle
        if self._is_location(loc):
          self._locations.append(loc)
    self._state_size = len(self._locations)

    # Neighbours - each line is a state, ranked by state-number, each column is a direction (N, E, S, W)
    self._neighbours = np.zeros((self._state_size, 4)) 
    
    for state in range(self._state_size):
      loc = self._get_loc_from_state(state)

      # North
      neighbour = (loc[0]-1, loc[1]) # North neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('N')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('N')] = state

      # East
      neighbour = (loc[0], loc[1]+1) # East neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('E')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('E')] = state

      # South
      neighbour = (loc[0]+1, loc[1]) # South neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('S')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('S')] = state

      # West
      neighbour = (loc[0], loc[1]-1) # West neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('W')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('W')] = state

    # Absorbing
    self._absorbing = np.zeros((1, self._state_size))
    for a in self._absorbing_locs:
      absorbing_state = self._get_state_from_loc(a)
      self._absorbing[0, absorbing_state] = 1

    # Transition matrix
    self._T = np.zeros((self._state_size, self._state_size, self._action_size)) # Empty matrix of domension S*S*A
    for action in range(self._action_size):
      for outcome in range(4): # For each direction (N, E, S, W)
        # The agent has prob_success probability to go in the correct direction
        if action == outcome:
          prob = 1 - 3.0 * ((1.0 - self._prob_success) / 3.0) # (theoritically equal to self.prob_success but avoid rounding error and garanty a sum of 1)
        # Equal probability to go into one of the other directions
        else:
          prob = (1.0 - self._prob_success) / 3.0
          
        # Write this probability in the transition matrix
        for prior_state in range(self._state_size):
          # If absorbing state, probability of 0 to go to any other states
          if not self._absorbing[0, prior_state]:
            post_state = self._neighbours[prior_state, outcome] # Post state number
            post_state = int(post_state) # Transform in integer to avoid error
            self._T[prior_state, post_state, action] += prob

    # Reward matrix
    self._R = np.ones((self._state_size, self._state_size, self._action_size)) # Matrix filled with 1
    self._R = self._default_reward * self._R # Set default_reward everywhere
    for i in range(len(self._absorbing_rewards)): # Set absorbing states rewards
      post_state = self._get_state_from_loc(self._absorbing_locs[i])
      self._R[:,post_state,:] = self._absorbing_rewards[i]

    # Creating the graphical Maze world
    self._graphics = GraphicsMaze(self._shape, self._locations, self._default_reward, self._obstacle_locs, self._absorbing_locs, self._absorbing_rewards, self._absorbing)
    
    # Reset the environment
    self.reset()


  def _is_location(self, loc):
    """
    Is the location a valid state (not out of Maze and not an obstacle)
    input: loc {tuple} -- location of the state
    output: _ {bool} -- is the location a valid state
    """
    if (loc[0] < 0 or loc[1] < 0 or loc[0] > self._shape[0]-1 or loc[1] > self._shape[1]-1):
      return False
    elif (loc in self._obstacle_locs):
      return False
    else:
      return True


  def _get_state_from_loc(self, loc):
    """
    Get the state number corresponding to a given location
    input: loc {tuple} -- location of the state
    output: index {int} -- corresponding state number
    """
    return self._locations.index(tuple(loc))


  def _get_loc_from_state(self, state):
    """
    Get the state number corresponding to a given location
    input: index {int} -- state number
    output: loc {tuple} -- corresponding location
    """
    return self._locations[state]

  # Getter functions used only for DP agents
  # You DO NOT NEED to modify them
  def get_T(self):
    return self._T

  def get_R(self):
    return self._R

  def get_absorbing(self):
    return self._absorbing

  # Getter functions used for DP, MC and TD agents
  # You DO NOT NEED to modify them
  def get_graphics(self):
    return self._graphics

  def get_action_size(self):
    return self._action_size

  def get_state_size(self):
    return self._state_size

  def get_gamma(self):
    return self._gamma

  # Functions used to perform episodes in the Maze environment
  def reset(self):
    """
    Reset the environment state to one of the possible starting states
    input: /
    output: 
      - t {int} -- current timestep
      - state {int} -- current state of the envionment
      - reward {int} -- current reward
      - done {bool} -- True if reach a terminal state / 0 otherwise
    """
    self._t = 0
    self._state = self._get_state_from_loc(self._starting_locs[random.randrange(len(self._starting_locs))])
    self._reward = 0
    self._done = False
    return self._t, self._state, self._reward, self._done

  def step(self, action):
    """
    Perform an action in the environment
    input: action {int} -- action to perform
    output: 
      - t {int} -- current timestep
      - state {int} -- current state of the envionment
      - reward {int} -- current reward
      - done {bool} -- True if reach a terminal state / 0 otherwise
    """

    # If environment already finished, print an error
    if self._done or self._absorbing[0, self._state]:
      print("Please reset the environment")
      return self._t, self._state, self._reward, self._done

    # Drawing a random number used for probaility of next state
    probability_success = random.uniform(0,1)

    # Look for the first possible next states (so get a reachable state even if probability_success = 0)
    new_state = 0
    while self._T[self._state, new_state, action] == 0: 
      new_state += 1
    assert self._T[self._state, new_state, action] != 0, "Selected initial state should be probability 0, something might be wrong in the environment."

    # Find the first state for which probability of occurence matches the random value
    total_probability = self._T[self._state, new_state, action]
    while (total_probability < probability_success) and (new_state < self._state_size-1):
     new_state += 1
     total_probability += self._T[self._state, new_state, action]
    assert self._T[self._state, new_state, action] != 0, "Selected state should be probability 0, something might be wrong in the environment."
    
    # Setting new t, state, reward and done
    self._t += 1
    self._reward = self._R[self._state, new_state, action]
    self._done = self._absorbing[0, new_state] or self._t > self._max_t
    self._state = new_state
    return self._t, self._state, self._reward, self._done


#################################################################################################################
"""## DP Agent"""

# This class define the Dynamic Programing agent 
# Task 1

class DP_agent(object):

  # [Action required]
  # WARNING: make sure this function can be called by the auto-marking script

  def value_iteration(self,env, gamma,threshold = 0.0001):

    epochs = 0
    delta = threshold 
    V = np.zeros(env.get_state_size())

    while delta >= threshold:

        epochs += 1 
        delta = 0

        for prior_state in range(env.get_state_size()):

            if not env.get_absorbing()[0, prior_state]:

                v = V[prior_state] 

                Q = np.zeros(4) 
                for post_state in range(env.get_state_size()):
                    Q += env.get_T()[prior_state, post_state,:] * (env.get_R()[prior_state, post_state, :] + gamma * V[post_state])

                V[prior_state]= np.max(Q) 

                delta = max(delta, np.abs(v - V[prior_state]))


    policy = np.zeros((env.get_state_size(), env.get_action_size())) 

    for prior_state in range(env.get_state_size()):
        Q = np.zeros(4)
        for post_state in range(env.get_state_size()):
            Q += env.get_T()[prior_state, post_state,:] * (env.get_R()[prior_state, post_state, :] + gamma * V[post_state])

        policy[prior_state, np.argmax(Q)] = 1 

    return policy, V, epochs
  
  def solve(self, env):
   
    """
    Solve a given Maze environment using Dynamic Programming
    input: env {Maze object} -- Maze to solve
    output: 
      - policy {np.array} -- Optimal policy found to solve the given Maze environment 
      - V {np.array} -- Corresponding value function 
    """
    
    """
    Task 1: Dynamic programming
        For comparison purposes, we are first going to assume we have access to the transition T 
        and reward function R of this environment and consider a Dynamic Programming agent to solve this problem.
        
    1.1.  Complete the solve() method of the DP_agent class. You can choose any Dynamic 
        Programming method and you are allowed to reuse code from the Gridworld notebook provided 
        as a reference. Note that for this agent only, you are allowed to access the transition matrix 
        through env.get_T(), the reward matrix through env.get_R() and the list of absorbing states through env.get_absorbing() 
        of the Maze class. You might also need to use env.get_action_size(), env.get_state_size() and env.get_gamma(). 
        You can define any additional methods inside the DP_agent class, but only the solve() method will be called for 
        script checking.DO NOT add any inputs or outputs to solve().

    1.2.  Please ensure that you state which method you choose to solve the problem and justify why you chose that method. Give and justify any parameters that you set, design that you chose or assumptions you made

    1.3.  Report the graphical representation of your optimal policy and optimal value function.

    1.4.  Discuss how the value of your γ and p have influenced the optimal value function and optimal policy in your personal Maze. In particular, you may investigate the effect of having a value of p < 0.25, p = 0.25 or p > 0.25, and similarly γ < 0.5 or γ > 0.5.

    
    """

    #### 
    # Add your code here
    # WARNING: for this agent only, you are allowed to access env.get_T(), env.get_R() and env.get_absorbing()
    ####

    gamma=env.get_gamma()

    
    def policy_evaluation(policy, threshold = 0.0001, gamma = env.get_gamma()):

      delta= 2*threshold 
      V = np.zeros(env.get_state_size())

      Vnew = np.copy(V) 
      epoch = 0
      
 
      while delta > threshold:
        epoch += 1

        for prior_state in range(env.get_state_size()):
          if not env.get_absorbing()[0, prior_state]: # Ensure state are non-absorbing

            temporay_V = 0 
            for action in range(env.get_action_size()):
              temporay_Q = 0 # Temporary variable for Q
              for post_state in range(env.get_state_size()):
                temporay_Q = temporay_Q + env.get_T()[prior_state, post_state, action] * (env.get_R()[prior_state, post_state, action] + gamma * V[post_state])
              temporay_V += policy[prior_state, action] * temporay_Q
                      
            Vnew[prior_state] = temporay_V
              
        delta =  max(abs(Vnew - V))
        V = np.copy(Vnew)

      return V,epoch
       
    def policy_iteration(threshold = 0.0001, gamma = env.get_gamma()):

      policy = np.zeros((env.get_state_size(), env.get_action_size())) 
      policy[:, 0] = 1 
      epochs = 0
      policy_stable = False 
      while not policy_stable: 
        
        V, epochs_eval = policy_evaluation(policy, threshold = threshold, gamma = gamma)
        epochs += epochs_eval

        policy_stable = True

        for prior_state in range(env.get_state_size()):
          if not env.get_absorbing()[0, prior_state]: 
                      
            old_action = np.argmax(policy[prior_state, :])
                  
            Q = np.zeros(env.get_action_size()) 
            for post_state in range(env.get_state_size()):
              Q += env.get_T()[prior_state, post_state, :] * (env.get_R()[prior_state, post_state, :] + gamma * V[post_state])

            new_policy = np.zeros(env.get_action_size())
            new_policy[np.argmax(Q)] = 1  
            
            policy[prior_state, :] = new_policy
                  
            if old_action != np.argmax(policy[prior_state, :]):
              policy_stable = False
              


      
      return policy, V, epochs
    
    policy_polic_iter, V_polic_iter, epochs_polic_iter = policy_iteration(threshold = 0.0001, gamma = env.get_gamma())

    policy_Value_iter, V_Value_iter, epochs_Value_iter = self.value_iteration(env,gamma = env.get_gamma(),threshold = 0.0001)

    #Comparing Policy iteration vs Value iteration
    # print(f'Comparing Policy Iteration Vs Value Iteration')

    # print(f'Epochs required for Policy iteration = {epochs_polic_iter}')
    # print(f'Epochs required for Value iteration = {epochs_Value_iter}')


    return policy_Value_iter, V_Value_iter

#######################################################################################

"""## MC agent"""

# This class define the Monte-Carlo agent
from collections import defaultdict

class MC_agent(object):
    from collections import defaultdict
    #I hope libaries can be imported in the automarking script because there was no indication stating we cannot use libraries

  
    def __init__(self) -> None:
        self.num_episodes = 5000 #initially set to 5000
        self.num_epochs = 30 #set at 30
        self.epsilon = 0.8 #initally set to 0.8
        self.alpha=0.1 #initially set to 0.1


    def make_epsilon_greedy_policy(self, Q, epsilon, nA):
        """
        Creates an epsilon-greedy policy based on a given Q-function and epsilon.
        
        Args:
            Q: A dictionary that maps from state -> action-values.
                Each value is a numpy array of length nA (see below)
            epsilon: The probability to select a random action . float between 0 and 1.
            nA: Number of actions in the environment.
        
        Returns:
            A function that takes the observation as an argument and returns
            the probabilities for each action in the form of a numpy array of length nA.
        
        """
        def policy_fn(observation):
            A = np.ones(nA, dtype=float) * epsilon / nA
            best_action = np.argmax(Q[observation])
            A[best_action] += (1.0 - epsilon)
            return A
        return policy_fn


# [Action required]
# WARNING: make sure this function can be called by the auto-marking script


    def solve(self, env):
    #I hope libaries can be imported in the automarking script because there was no indication stating we cannot use libraries

        """
        Solve a given Maze environment using Monte Carlo learning
        input: env {Maze object} -- Maze to solve
        output: 
        - policy {np.array} -- Optimal policy found to solve the given Maze environment 
        - values {list of np.array} -- List of successive value functions for each episode 
        - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode 
        """
        """
        Task 2: Monte-Carlo Reinforcement Learning

        We are now assuming that we do not have access to the transition T and reward function R 
        of this environment. We want to solve this problem using a Monte-Carlo learning agent.

            2.1.  Complete the solve() method of the MC_agent class. Note that for this agent you are only
            allowed to use the env.reset() and env.step() methods of the Maze class, as well as env.get_action_size(),
            env.get_state_size() and env.get_gamma(). DO NOT use the transition matrix env.get_T(), the reward matrix env.get_R()
            and the list of absorbing states env.get_absorbing(). You can define any additional methods inside the 
            MC_agent class, but only the solve() method will be called for script checking. DO NOT add any inputs or outputs to solve().
            
            2.2.  State which method you choose to solve the problem and justify your choice. 
            Give and justify any parameters that you set, design that you chose or assumptions you made
            
            2.3. Report the graphical representation of your optimal policy and optimal value function.
            
            2.3. Multiple MC-learning runs will lead to slightly different results due to variability in
            the learning, and you would need to replicate the learning process multiple times to get an idea
            of the true performance of your agent. Identify the sources of this variability, and state what
            a ”sufficient” number of replications would be, explaining how you would choose it.
            2.4. Plot the learning curve of your agent: the total non-discounted sum of reward against the number of episodes. The figure should picture the mean and standard deviation across your replications on the same graph (using the replication number you stated in the previous question).
            2.5. How does varying the exploration parameter ϵ and the learning rate α of your algorithm impact your learning curves? Briefly explain what you find and relate it where possible to the theory you learned. We encouraged you to use relevant additional plots to illustrate your findings in this question

        """

        # Initialisation (can be edited)
        from collections import defaultdict

        Q = np.random.rand(env.get_state_size(), env.get_action_size()) 
        # Q = np.zeros((env.get_state_size(), env.get_action_size()))
        policy = np.zeros((env.get_state_size(), env.get_action_size()))
        # V = np.zeros(env.get_state_size())
        # values =[V]
 


        policy_func = self.make_epsilon_greedy_policy(Q, self.epsilon, env.get_action_size())
        optimal_policy_func = self.make_epsilon_greedy_policy(Q, self.epsilon, env.get_action_size())
        total_rewards =  np.zeros((self.num_epochs,self.num_episodes))
        values= np.zeros((self.num_epochs,self.num_episodes,env.get_state_size()))
        for epoch in range(self.num_epochs):
            returns_sum = defaultdict(float)
            returns_count = defaultdict(float)
            # print("\rReplication {}/{}.".format(epoch, self.num_epochs), end="")

            for i_episode in range(1, self.num_episodes + 1):
                if i_episode % 1000 == 0:
                  print("\r Replication {}/{} Episode {}/{}.".format(epoch+1,self.num_epochs,i_episode, self.num_episodes),end="")


                # Generate an episode.
                # An episode is an array of (state, action, reward) tuples
                episode = []
                _, state, _, _ = env.reset()
                done = False
                episode_reward = 0

                while not done:
                    probs = policy_func(state)
                    action = np.random.choice(np.arange(len(probs)), p=probs)
                    policy[state] = optimal_policy_func(state)
                    _, next_state, reward, done = env.step(action)
                    episode_reward += reward
                    episode.append((state, action, reward))
                    state = next_state
                # Find all (state, action) pairs we've visited in this episode
                # We convert each state to a tuple so that we can use it as a dict key

                sa_in_episode = set([(tuple([x[0], x[1]])) for x in episode])
                for state, action in sa_in_episode:
                    sa_pair = (state, action)
                    # Find the first occurance of the (state, action) pair in the episode
                    first_occurence_idx = next(i for i,x in enumerate(episode)
                                            if x[0] == state and x[1] == action)
                    # Sum up all rewards since the first occurance
                    G = sum([x[2]*(env.get_gamma()**i) for i,x in enumerate(episode[first_occurence_idx:])])
                    # Calculate average return for this state over all sampled episodes
                    returns_sum[sa_pair] += G
                    returns_count[sa_pair] += 1.0
                    Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]
            
                total_rewards[epoch][i_episode-1] = episode_reward
                # values.append([Q.max(axis=1)])
                values[epoch][i_episode-1][:]=Q.max(axis=1)
        #### 
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        
        return policy, values[-1], total_rewards
########################################################################################

"""## TD agent"""

# This class define the Temporal-Difference agent
class TD_agent(object):

    def __init__(self) -> None:
        self.num_episodes = 5000 #initially set to 2000
        
        # Episode: All states that come in between an initial-state and a terminal-state


        self.num_epochs = 30 #initially set to 30

        self.epsilon = 0.8 #initially set to 0.8
        
        #In the Epsilon-Greedy Exploration strategy, 
        #the agent chooses a random action with probability epsilon 
        #and exploits the best known action with probability 1 — epsilon.


        self.alpha=0.1 #initially set to 0.5

        #alpha is the learning rate  - the learning rate, set between 0 and 1. 
        #Setting it to 0 means that the Q-values are never updated, hence nothing is learned. 
        #Setting a high value such as 0.9 means that learning can occur quickly.


    def make_epsilon_greedy_policy(self, Q, epsilon, nA):
        """
        Creates an epsilon-greedy policy based on a given Q-function and epsilon.
        
        Args:
            Q: A dictionary that maps from state -> action-values.
                Each value is a numpy array of length nA (see below)
            epsilon: The probability to select a random action. Float between 0 and 1.
            nA: Number of actions in the environment.
        
        Returns:
            A function that takes the observation as an argument and returns
            the probabilities for each action in the form of a numpy array of length nA.
        
        """
        def policy_fn(observation):
            A = np.ones(nA, dtype=float) * epsilon / nA
            best_action = np.argmax(Q[observation])
            A[best_action] += (1.0 - epsilon)
            return A
        return policy_fn



    # [Action required]
    # WARNING: make sure this function can be called by the auto-marking script
    def solve(self, env):
        """
        Solve a given Maze environment using Temporal Difference learning
        input: env {Maze object} -- Maze to solve
        output: 
        - policy {np.array} -- Optimal policy found to solve the given Maze environment 
        - values {list of np.array} -- List of successive value functions for each episode 
        - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode 
        """
        """
        Task 3: Temporal Difference Reinforcement Learning
        
        3.1.	Complete the solve() method of the TD_agent class. Note that for this agent 
        you are only allowed to use the env.reset() and env.step() methods of the Maze class,
        as well as env.get_action_size(), env.get_state_size() and env.get_gamma(). DO NOT use 
        the transition matrix env.get_T(), the reward matrix env.get_R() and the list of absorbing 
        states env.get_absorbing(). You can define any additional methods inside the TD_agent class,
        but only the solve() method will be called for automatic script checking. DO NOT add any inputs 
        or outputs to solve().

        3.2.	State which method you choose to solve the problem and justify your choice.
        Give and justify any parameters that you set, design that you chose or assumptions you made.

        3.3.	Report the graphical representation of your optimal policy and optimal value function

        3.4.	Plot the learning curve of your agent: the total non-discounted sum of reward
        against the number of episodes. The figure should picture the mean and standard deviation
        across your replications on the same graph (using the replication number you stated in 
        Question 2: MonteCarlo Reinforcement Learning).

        3.5.	How does varying the exploration parameter ϵ and the learning
        rate α of your algorithm impact your learning curves? Briefly explain 
        what you find and relate it where possible to the theory you learned. 
        We encouraged you to use relevant additional plots to illustrate your findings in this question

        """
        

        # Initialisation (can be edited)
        Q = np.random.rand(env.get_state_size(), env.get_action_size()) 
        policy = np.zeros((env.get_state_size(), env.get_action_size())) 

        policy_func = self.make_epsilon_greedy_policy(Q, self.epsilon, env.get_action_size())
        optimal_policy_func = self.make_epsilon_greedy_policy(Q, 0, env.get_action_size())
        total_rewards = np.zeros((self.num_epochs,self.num_episodes))
        values= np.zeros((self.num_epochs,self.num_episodes,env.get_state_size()))
    
        for epoch in range(self.num_epochs):
            for i_episode in range(self.num_episodes):
                if i_episode % 1000 == 0:
                  print("\r Replication {}/{} Episode {}/{}.".format(epoch+1,self.num_epochs,i_episode, self.num_episodes),end="")
                
                # Reset the environment and pick the first action
                _, state, _, _ = env.reset()
                done = False
                episode_reward = 0
                
                # One step in the environment
                while not done:
                    
                    # Take a step
                    action_probs = policy_func(state)
                    action = np.random.choice(np.arange(len(action_probs)), p=action_probs)
                    policy[state] = optimal_policy_func(state)

                    _, next_state, reward, done = env.step(action)

                    # Update statistics
                    episode_reward += reward
                    
                    # TD Update
                    best_next_action = np.argmax(Q[next_state])    
                    td_target = reward + env.get_gamma() * Q[next_state][best_next_action]
                    td_delta = td_target - Q[state][action]
                    Q[state][action] += self.alpha * td_delta
                        
                    state = next_state
                total_rewards[epoch][i_episode] = episode_reward

                values[epoch][i_episode][:]= Q.max(axis=1)

        #### 
        # Add your code here
        # WARNING: this agent only has access to env.reset() and env.step()
        # You should not use env.get_T(), env.get_R() or env.get_absorbing() to compute any value
        ####
        

        return policy, values[-1], total_rewards

#############################
if __name__ == '__main__':

  """## Question 1: Dynamic Programming

  #### Question 1.1

  See Report

  #### Question 1.2
  """

  ### Question 0: Defining the environment

  print("Creating the Maze:\n")
  maze = Maze()
  from re import A


  ### Question 1: Dynamic programming

  dp_agent = DP_agent() #Initalizes dynamic programming agent
  dp_policy_Value_iter, dp_V_Value_iter=dp_agent.solve(maze) #obtains optimal policy and value for each state


  # print("Results of the DP agent:\n")
  maze.get_graphics().draw_policy(dp_policy_Value_iter)
  maze.get_graphics().draw_value(dp_V_Value_iter)

  # """
  # N.B within the dp_agent class,  policy iteration is implemented in a seperate method to show a comparasion between value and policy iteration.
  # It was concluded that Value iteration is better and hence the optimal policy and value function is obtained using value iteration
  # """

  # """#### Question 1.3"""

  gamma_range = [0, 0.2, 0.4, 0.6, 0.8]
  epochs = []
  values = []
  titles = []
  policies = []


  # Use Value iteration for each gamma value
  for gamma in gamma_range:
      policy_Value_iter,V, epoch = dp_agent.value_iteration(maze,gamma, threshold = 0.001)
      epochs.append(epoch)
      values.append(V)
      policies.append(policy_Value_iter)

      titles.append("gamma = {}".format(gamma))
  # Plot the number of epochs vs gamma values
  print("Impact of gamma value on the number of epochs needed for the Value Iteration algorithm:\n")
  plt.figure()
  plt.plot(gamma_range, epochs)
  plt.xlabel("Gamma")
  plt.ylabel("Number of epochs")
  plt.show()


  # Print all value functions and policies for different values of gamma
  print("\nGraphical representation of the value function for each gamma:\n")
  maze.get_graphics().draw_value_grid(values,titles, 1, 6)

  # print("\nGraphical representation of the policy for each gamma:\n")
  maze.get_graphics().draw_policy_grid(policies,titles, 1, 6)

  value_p=[]
  policy_p=[]
  titles_p=[]

  #Question 1.3
  maze = Maze()
  maze._prob_success=0.5


  dp_agent = DP_agent()
  print(f' from cell = {maze._prob_success}')
  dp_policy_Value_iter, dp_V_Value_iter=dp_agent.solve(maze)


  # print("Results of the DP agent:\n")
  maze.get_graphics().draw_policy(dp_policy_Value_iter)
  maze.get_graphics().draw_value(dp_V_Value_iter)

  value_p.append(dp_V_Value_iter)
  policy_p.append(dp_policy_Value_iter)
  titles_p.append("p = {}".format(maze._prob_success))

  # Print all value functions and policies for different values of gamma
  print("\nGraphical representation of the value function for each P:\n")
  maze.get_graphics().draw_value_grid(value_p,titles_p, 1, 6)

  # print("\nGraphical representation of the policy for each gamma:\n")
  maze.get_graphics().draw_policy_grid(policy_p,titles_p, 1, 6)

  """## Question *2*: Monte Carlo

  #### Question 2.1

  See report
  """

  # #Question 2.1

  #NB I included my batch iteration within my def sol, so if u wish to run the code below please go to the class and remove the [-1] next to the return values output
  # Finding Optimal Epsilon Value
  maze = Maze()
  from re import A
  mc_agent = MC_agent()
  mc_policy, mc_values, total_rewards = mc_agent.solve(maze)

  Epsilon_range=[0.2,0.5,0.8]
  plt.figure(figsize=(10,10))
  MC_Values_list=[]
  MC_Policy_list=[]
  MC_titles_list=[]

  print(f'Varing Epsilon Rate')
  for Epsilon in Epsilon_range:
    mc_agent.epsilon=Epsilon
    mc_policy, mc_values, total_rewards = mc_agent.solve(maze)
    print(f"Epsilon = {mc_agent.epsilon}:\n")
    # maze.get_graphics().draw_policy(td_policy)
    # maze.get_graphics().draw_value(td_values[-1][-1])
    MC_Values_list.append(mc_values[-1][-1])
    MC_Policy_list.append(mc_policy)
    MC_titles_list.append("Epsilon = {}".format(Epsilon))

    # Reward plot
    # plt.plot(np.arange(mc_agent.num_episodes), total_rewards.mean(axis=0),label=f'Epsilon = {mc_agent.epsilon}')


  # Print all value functions and policies for different values of gamma
  # print("\nGraphical representation of the value function for each P:\n")
  # maze.get_graphics().draw_value_grid(MC_Values_list,MC_titles_list, 1, 6)

  # # print("\nGraphical representation of the policy for each gamma:\n")
  # maze.get_graphics().draw_policy_grid(MC_Policy_list,MC_titles_list, 1, 6)

  """#### Question 2.2"""

  ## Question 2.2: Monte-Carlo learning
  print("Creating the Maze:\n")
  maze = Maze()
  from re import A
  mc_agent = MC_agent()
  mc_policy, mc_values, total_rewards = mc_agent.solve(maze)

  print("Results of the MC agent:\n")
  maze.get_graphics().draw_policy(mc_policy)
  maze.get_graphics().draw_value(mc_values[-1])

  """#### Question 2.3

  See report

  #### Question 2.4
  """

  # # Question 2.4
  # # Reward plot for MC 
  plt.figure(figsize=(10,10))
  num_episodes=5000
  plt.plot(np.arange(num_episodes), total_rewards.mean(axis=0)[:num_episodes],label='mean total discounted reward',color='Green')
  plt.fill_between(np.arange(num_episodes), total_rewards.mean(axis=0)[:num_episodes]-total_rewards.std(axis=0)[:num_episodes], total_rewards.mean(axis=0)[:num_episodes]+total_rewards.std(axis=0)[:num_episodes],label='std',color='lightgreen')
  plt.xlabel("Episode")
  plt.ylabel("Episode Reward")
  plt.title("Learning Curve")
  plt.legend()
  plt.show()

  # """#### Question 2.5"""

  #Question 2.5

  Epsilon_range=[0.1,0.5,0.9]
  plt.figure(figsize=(10,10))
  mc_agent = MC_agent()

  print(f'Varing Epsilon Rate')
  for Epsilon in Epsilon_range:
    mc_agent.epsilon=Epsilon
    mc_policy_iter, mc_values_iter, total_rewards_mc_iter = mc_agent.solve(maze)
    # Reward plot
    plt.plot(np.arange(mc_agent.num_episodes), total_rewards_mc_iter.mean(axis=0),label=f'Epsilon = {mc_agent.epsilon}')

  plt.xlabel("Episode")
  plt.ylabel("Episode Reward")
  plt.title("Learning Curve")
  plt.legend()
  plt.show()

  # """N.B. The graph for varying Alpha was not required since Monte Carlo does not use Alpha

  # ## Question 3

  #### Question 3.1

  # See report

  #### Question 3.2
  

  ### Question 3.2: Temporal-Difference learning

  print("Creating the Maze:\n")
  maze = Maze()
  from re import A

  td_agent = TD_agent()
  td_policy, td_values, td_total_rewards = td_agent.solve(maze)

  # print("Results of the TD agent:\n")
  # maze.get_graphics().draw_policy(td_policy)
  # maze.get_graphics().draw_value(td_values[-1])

  # # """#### Question 3.3"""

  # # #Question 3.3
  # from matplotlib import legend
  # # Reward plot for TD 
  # plt.figure(figsize=(10,10))
  # plt.plot(np.arange(td_agent.num_episodes), td_total_rewards.mean(axis=0),label='mean total non-discounted reward',color='blue')
  # plt.fill_between(np.arange(td_agent.num_episodes), td_total_rewards.mean(axis=0)-td_total_rewards.std(axis=0), td_total_rewards.mean(axis=0)+td_total_rewards.std(axis=0),label='std',color='lightblue')
  # plt.xlabel("Episode")
  # plt.ylabel("Episode Reward")
  # plt.legend()
  # plt.title("Learning Curve")

  # # """#### Question 3.4"""

  # # #Question 3.4
  # # Finding Optimal Epsilon Value
  # Epsilon_range=[0.1,0.5,0.9]
  # plt.figure(figsize=(10,10))
  # td_agent = TD_agent()
  # TD_Values_list=[]
  # TD_Policy_list=[]
  # TD_titles_list=[]

  # print(f'Varing Epsilon Rate')
  # for Epsilon in Epsilon_range:
  #   td_agent.epsilon=Epsilon
  #   td_policy_iter, td_values_iter, total_rewards_iter = td_agent.solve(maze)
  #   print(f"Epsilon = {td_agent.epsilon}:\n")
  #   # maze.get_graphics().draw_policy(td_policy)
  #   # maze.get_graphics().draw_value(td_values[-1][-1])
  #   TD_Values_list.append(td_values[-1][-1])
  #   TD_Policy_list.append(td_policy)
  #   TD_titles_list.append("Epsilon = {}".format(Epsilon))

  #   # Reward plot
  #   plt.plot(np.arange(td_agent.num_episodes), total_rewards_iter.mean(axis=0),label=f'Epsilon = {td_agent.epsilon}')


  # plt.xlabel("Episode")
  # plt.ylabel("Episode Reward")
  # plt.title("Learning Curve")
  # plt.legend()
  # plt.show()

  # # Print all value functions and policies for different values of gamma
  # print("\nGraphical representation of the value function for each P:\n")
  # maze.get_graphics().draw_value_grid(TD_Values_list,TD_titles_list, 1, 6)

  # # print("\nGraphical representation of the policy for each gamma:\n")
  # maze.get_graphics().draw_policy_grid(TD_Policy_list,TD_titles_list, 1, 6)

  # print(f'Varing Alpha')
  # Alpha_range=[0.1,0.5,1]
  # plt.figure(figsize=(10,10))
  # td_agent = TD_agent()

  # for Alpha in Alpha_range:
  #   td_agent.alpha=Alpha
  #   td_policy, td_values, total_rewards = td_agent.solve(maze)
  #   print(f"Alpha = {td_agent.alpha}:\n")

  #   # Reward plot
  #   plt.plot(np.arange(td_agent.num_episodes), total_rewards.mean(axis=0),label=f'Alpha = {td_agent.alpha}')
  #   # plt.fill_between(np.arange(td_agent.num_episodes), total_rewards.mean(axis=0)-total_rewards.std(axis=0), total_rewards.mean(axis=0)+total_rewards.std(axis=0))


  # plt.xlabel("Episode")
  # plt.ylabel("Episode Reward")
  # plt.title("Learning Curve")
  # plt.legend()
  # plt.show()

  # # """# Question 4"""

  # from sklearn.metrics import mean_squared_error


  # MSE_TD_array=np.zeros((td_agent.num_epochs,td_agent.num_episodes))

  # for x in range(0,td_agent.num_epochs,1):
  #   for y in range(0,td_agent.num_episodes,1):
  #     MSE_TD_array[x][y]=mean_squared_error(td_values[x][y],dp_V_Value_iter)  

  # MSE_MC_array=np.zeros((mc_agent.num_epochs,mc_agent.num_episodes))

  # for x in range(0,mc_agent.num_epochs,1):
  #   for y in range(0,mc_agent.num_episodes,1):
  #     MSE_MC_array[x][y]=mean_squared_error(mc_values[x][y],dp_V_Value_iter)

  # # #Question 3.3
  # from matplotlib import legend
  # # Reward plot for TD 
  # plt.figure(figsize=(10,10))
  # num_episodes=500

  # plt.plot(np.arange(num_episodes), MSE_TD_array.mean(axis=0)[:num_episodes],label='MSE Temporal difference',color='blue')
  # plt.fill_between(np.arange(num_episodes), MSE_TD_array.mean(axis=0)[:num_episodes]-MSE_TD_array.std(axis=0)[:num_episodes], MSE_TD_array.mean(axis=0)[:num_episodes]+MSE_TD_array.std(axis=0)[:num_episodes],color='lightblue')


  # plt.plot(np.arange(num_episodes), MSE_MC_array.mean(axis=0)[:num_episodes],label='MSE Monte Carlo',color='green')
  # plt.fill_between(np.arange(num_episodes), MSE_MC_array.mean(axis=0)[:num_episodes]-MSE_MC_array.std(axis=0)[:num_episodes], MSE_MC_array.mean(axis=0)[:num_episodes]+MSE_MC_array.std(axis=0)[:num_episodes],color='lightgreen')



  # plt.xlabel("Episode")
  # plt.ylabel("MSE")
  # plt.legend()
  # plt.title("Function estimation error")
  # plt.show()

  # """#### Question 4.3"""

  # #TD Scatter
  # plt.figure(figsize=(7,7))
  # plt.scatter(td_total_rewards[-1], MSE_TD_array[-1], c ="blue",marker ="^",s=7,label='Q learning')


  # plt.xlabel("Total none discounted reward")
  # plt.ylabel("Function estimation error")
  # plt.legend()
  # plt.show()

  # #TD Scatter
  # plt.figure(figsize=(7,7))

  # plt.scatter(total_rewards.mean(axis=0), MSE_MC_array.mean(axis=0)-10000, c ="green",marker ="s",s=7,label='Monte Carlo')


  # plt.xlabel("Total none discounted reward")
  # plt.ylabel("Function estimation error")
  # plt.legend()
  # plt.show()